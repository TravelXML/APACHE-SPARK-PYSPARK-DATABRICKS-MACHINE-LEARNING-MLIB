{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b43d0d54-036b-475f-994c-6eff4d5a7d28",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Examples Of Pyspark ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa29c53d-bc83-47ff-8821-d82bc35cbf55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('Missing').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d332a20-ffd4-4150-9f42-0d44587db6e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Read The dataset\n",
    "training = spark.read.csv('/FileStore/shared_uploads/astartupcto@gmail.com/test1.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21d47ff6-8422-43f4-8474-d837ff154649",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+------+\n|    Name|Age|Experience|Salary|\n+--------+---+----------+------+\n|   Sapan| 31|        10| 30000|\n|Priyanka| 30|         8| 25000|\n|Gurpreet| 29|         4| 20000|\n|   Payal| 24|         3| 20000|\n|   Priya| 21|         1| 15000|\n|   Aayan| 23|         2| 18000|\n+--------+---+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "training.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf063464-4e2f-44ad-bb86-bb90db4c2d0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Name: string (nullable = true)\n |-- Age: integer (nullable = true)\n |-- Experience: integer (nullable = true)\n |-- Salary: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "training.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56824ee6-91c5-4159-8fbd-ae302c458d10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[18]: ['Name', 'Age', 'Experience', 'Salary']"
     ]
    }
   ],
   "source": [
    "training.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d430dfce-edc6-40de-beae-b3041fe1422a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#The purpose of using VectorAssembler is to transform multiple feature columns into a single feature vector column. \n",
    "# This is a common preprocessing step in machine learning pipelines. \n",
    "# By combining multiple features into a single vector column, it becomes easier to use them as input for machine learning models in PySpark's MLlib\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "featureassembler=VectorAssembler(inputCols=[\"Age\",\"Experience\"],outputCol=\"Independent Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16b6a57a-d3a7-4e41-ad40-ce8b4f243ce7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#The purpose of this transformation is to prepare the training data for machine learning modeling. \n",
    "# By combining multiple feature columns into a single vector column, you can easily use this vector as input \n",
    "# for various machine learning algorithms in PySpark's MLlib.\n",
    "output=featureassembler.transform(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74877273-de3d-44cc-b619-5c458aea1f2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+------+--------------------+\n|    Name|Age|Experience|Salary|Independent Features|\n+--------+---+----------+------+--------------------+\n|   Sapan| 31|        10| 30000|         [31.0,10.0]|\n|Priyanka| 30|         8| 25000|          [30.0,8.0]|\n|Gurpreet| 29|         4| 20000|          [29.0,4.0]|\n|   Payal| 24|         3| 20000|          [24.0,3.0]|\n|   Priya| 21|         1| 15000|          [21.0,1.0]|\n|   Aayan| 23|         2| 18000|          [23.0,2.0]|\n+--------+---+----------+------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a4975cc-7d5e-404e-b4ab-fec8662df39a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[22]: ['Name', 'Age', 'Experience', 'Salary', 'Independent Features']"
     ]
    }
   ],
   "source": [
    "output.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "745e987f-0bf7-47df-8672-09a3f8642cdc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "finalized_data=output.select(\"Independent Features\",\"Salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3ed85df-7bbe-4ee5-981c-f84dd085a96d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n|Independent Features|Salary|\n+--------------------+------+\n|         [31.0,10.0]| 30000|\n|          [30.0,8.0]| 25000|\n|          [29.0,4.0]| 20000|\n|          [24.0,3.0]| 20000|\n|          [21.0,1.0]| 15000|\n|          [23.0,2.0]| 18000|\n+--------------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "finalized_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa7c955b-4481-4b9d-8533-ffc78075cf71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This line imports the LinearRegression class from the pyspark.ml.regression module. \n",
    "# LinearRegression is a machine learning algorithm used for predicting a continuous target variable based on one or more input features.\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "##train test split\n",
    "#finalized_data: This is the DataFrame containing your data after transformations (e.g., with the Independent Features vector column).\n",
    "#randomSplit([0.75, 0.25]): This method splits the data into training and test datasets. 75% of the data will be used for training the model (train_data), and the remaining #25% will be used for testing the model (test_data).\n",
    "train_data,test_data=finalized_data.randomSplit([0.75,0.25])\n",
    "#Creating and Training the Linear Regression Model\n",
    "regressor=LinearRegression(featuresCol='Independent Features', labelCol='Salary')\n",
    "regressor=regressor.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cceeada-c255-4d25-ba4d-2ac6f6744c8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[26]: DenseVector([109.3058, 1199.4092])"
     ]
    }
   ],
   "source": [
    "### Coefficients\n",
    "regressor.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9cd0b59-6338-4be2-a270-3fd443e6c586",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[27]: 12187.59231905408"
     ]
    }
   ],
   "source": [
    "### Intercepts\n",
    "regressor.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97998bb0-0dbe-49dc-b3ff-4430659b05ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Prediction\n",
    "pred_results=regressor.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28f70f14-ee97-4f56-9ad1-c1b0601880d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------------------+\n|Independent Features|Salary|        prediction|\n+--------------------+------+------------------+\n|          [24.0,3.0]| 20000|18409.158050221544|\n|         [31.0,10.0]| 30000|27570.162481536125|\n+--------------------+------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "pred_results.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48836984-0fda-4cf7-b296-0b37b4603dc1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[30]: (2010.3397341211657, 4217444.237654801)"
     ]
    }
   ],
   "source": [
    "pred_results.meanAbsoluteError,pred_results.meanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57e5dc07-1364-40f1-a685-608c54c8b7d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-102.5299600532517,1688.6817576564458]\nIntercept: 16470.039946737463\nRMSE: 666.648912\nR2: 0.982531\n+---+----------+------+------------------+\n|Age|Experience|Salary|        prediction|\n+---+----------+------+------------------+\n| 30|         8| 25000|26903.595206391477|\n+---+----------+------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Step 1: Create a Spark Session\n",
    "spark = SparkSession.builder.appName('LinearRegressionExample').getOrCreate()\n",
    "\n",
    "# Step 2: Define the Schema (optional, for explicit schema definition)\n",
    "# If schema is not defined, Spark will infer it automatically\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True),\n",
    "    StructField(\"Experience\", IntegerType(), True),\n",
    "    StructField(\"Salary\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Step 3: Load Data from File Path\n",
    "file_path = '/FileStore/shared_uploads/astartupcto@gmail.com/test1.csv'\n",
    "df = spark.read.csv(file_path, schema=schema, header=True)\n",
    "\n",
    "# Step 4: Clean and Transform Data\n",
    "# Drop rows with null values\n",
    "df = df.dropna()\n",
    "\n",
    "# Define the VectorAssembler\n",
    "featureassembler = VectorAssembler(inputCols=[\"Age\", \"Experience\"], outputCol=\"Independent Features\")\n",
    "\n",
    "# Apply the VectorAssembler to the DataFrame\n",
    "finalized_data = featureassembler.transform(df)\n",
    "\n",
    "# Step 5: Train-Test Split\n",
    "train_data, test_data = finalized_data.randomSplit([0.75, 0.25])\n",
    "\n",
    "# Step 6: Create and Train the Linear Regression Model\n",
    "regressor = LinearRegression(featuresCol='Independent Features', labelCol='Salary')\n",
    "regressor = regressor.fit(train_data)\n",
    "\n",
    "# Show the training summary\n",
    "training_summary = regressor.summary\n",
    "print(\"Coefficients: \" + str(regressor.coefficients))\n",
    "print(\"Intercept: \" + str(regressor.intercept))\n",
    "print(\"RMSE: %f\" % training_summary.rootMeanSquaredError)\n",
    "print(\"R2: %f\" % training_summary.r2)\n",
    "\n",
    "# Step 7: Make Predictions on the Test Data\n",
    "predictions = regressor.transform(test_data)\n",
    "predictions.select(\"Age\", \"Experience\", \"Salary\", \"prediction\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ab04bd4-9c91-40af-aa54-68ca53850a48",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's dive into training summary:\n",
    "\n",
    "### Coefficients\n",
    "\n",
    "```plaintext\n",
    "Coefficients: [-102.5299600532517, 1688.6817576564458]\n",
    "```\n",
    "\n",
    "- **Coefficients** are the weights assigned to the features in your linear regression model. In this case, you have two features: `Age` and `Experience`.\n",
    "\n",
    "  - The first coefficient `-102.5299600532517` is associated with the `Age` feature.\n",
    "  - The second coefficient `1688.6817576564458` is associated with the `Experience` feature.\n",
    "\n",
    "  These coefficients indicate how much the target variable (`Salary`) is expected to change with a one-unit change in the corresponding feature, holding all other features constant.\n",
    "\n",
    "  - A coefficient of `-102.5299600532517` for `Age` means that, on average, for each additional year of age, the salary is expected to decrease by approximately 102.53 units, assuming `Experience` remains constant.\n",
    "  - A coefficient of `1688.6817576564458` for `Experience` means that, on average, for each additional year of experience, the salary is expected to increase by approximately 1688.68 units, assuming `Age` remains constant.\n",
    "\n",
    "### Intercept\n",
    "\n",
    "```plaintext\n",
    "Intercept: 16470.039946737463\n",
    "```\n",
    "\n",
    "- The **Intercept** is the expected value of the target variable (`Salary`) when all the features (`Age` and `Experience`) are zero. It represents the baseline level of the target variable without any influence from the features.\n",
    "\n",
    "  In this case, an intercept of `16470.039946737463` means that if both `Age` and `Experience` were zero, the model would predict a salary of approximately 16,470.04 units.\n",
    "\n",
    "### RMSE (Root Mean Squared Error)\n",
    "\n",
    "```plaintext\n",
    "RMSE: 666.648912\n",
    "```\n",
    "\n",
    "- **RMSE** is a measure of the differences between the predicted values and the actual values. It is the square root of the average of the squared differences between predicted and actual values. It gives an idea of how well the model's predictions match the actual data.\n",
    "\n",
    "  An RMSE of `666.648912` means that, on average, the model's predictions are off by about 666.65 units from the actual salaries. Lower RMSE values indicate better model performance.\n",
    "\n",
    "### R² (R-Squared)\n",
    "\n",
    "```plaintext\n",
    "R2: 0.982531\n",
    "```\n",
    "\n",
    "- **R² (R-Squared)** is a statistical measure that represents the proportion of the variance for the target variable that is explained by the features in the model. It ranges from 0 to 1.\n",
    "\n",
    "  An R² of `0.982531` means that approximately 98.25% of the variance in the salary can be explained by the `Age` and `Experience` features in the model. Higher R² values indicate a better fit of the model to the data.\n",
    "\n",
    "### Summary\n",
    "\n",
    "To summarize:\n",
    "\n",
    "- **Coefficients** tell us how much the target variable is expected to change with a one-unit change in the corresponding feature.\n",
    "- **Intercept** represents the expected value of the target variable when all features are zero.\n",
    "- **RMSE** indicates the average error of the model's predictions.\n",
    "- **R²** shows how well the features explain the variance in the target variable.\n",
    "\n",
    "These metrics help us understand the performance and characteristics of the linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9aa923f-4cfe-4748-839b-4efbc885c089",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PYSPARK ML",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
